var documenterSearchIndex = {"docs":
[{"location":"public_api.html#API-walkthrough","page":"Public API","title":"API walkthrough","text":"","category":"section"},{"location":"public_api.html","page":"Public API","title":"Public API","text":"The function derivative_estimate transforms a stochastic program containing discrete randomness into a new program whose average is the derivative of the original.","category":"page"},{"location":"public_api.html","page":"Public API","title":"Public API","text":"derivative_estimate","category":"page"},{"location":"public_api.html#StochasticAD.derivative_estimate","page":"Public API","title":"StochasticAD.derivative_estimate","text":"derivative_estimate(X, p; backend=StochasticAD.PrunedFIs)\n\nCompute an unbiased estimate of fracmathrmdmathbbEX(p)mathrmdp, the derivative of the expectation of the real-valued random function X(p)  with respect to its input p, where p <: Real or p <: AbstractVector. The backend keyword argument describes the algorithm used by the third component of the stochastic triple, see technical details for more details.\n\n\n\n\n\n","category":"function"},{"location":"public_api.html","page":"Public API","title":"Public API","text":"While derivative_estimate is self-contained, we can also use the functions below to work with stochastic triples directly.","category":"page"},{"location":"public_api.html","page":"Public API","title":"Public API","text":"StochasticAD.stochastic_triple\nStochasticAD.derivative_contribution\nStochasticAD.value\nStochasticAD.delta\nStochasticAD.perturbations","category":"page"},{"location":"public_api.html#StochasticAD.stochastic_triple","page":"Public API","title":"StochasticAD.stochastic_triple","text":"stochastic_triple(X, p; backend=StochasticAD.PrunedFIs)\nstochastic_triple(p; backend=StochasticAD.PrunedFIs)\n\nIf p <: Real, return the result of propagating the stochastic triple p + Œµ through the random function X(p). If p <: AbstractVector, return a vector of stochastic triples of the same shape as p, containing the stochastic triples that result from perturbing the corresponding array elements of p one-by-one. When X is not provided, the identity function is used. The backend keyword argument describes the algorithm  used by the third component of the stochastic triple, see technical details for more details.\n\n\n\n\n\n","category":"function"},{"location":"public_api.html#StochasticAD.derivative_contribution","page":"Public API","title":"StochasticAD.derivative_contribution","text":"derivative_contribution(st::StochasticTriple)\n\nReturn the derivative estimate given by combining the dual and triple components of st.\n\n\n\n\n\n","category":"function"},{"location":"public_api.html#StochasticAD.value","page":"Public API","title":"StochasticAD.value","text":"value(st::StochasticTriple)\n\nReturn the primal value of st.\n\n\n\n\n\n","category":"function"},{"location":"public_api.html#StochasticAD.delta","page":"Public API","title":"StochasticAD.delta","text":"delta(st::StochasticTriple)\n\nReturn the almost-sure derivative of st, i.e. the rate of infinitesimal change.\n\n\n\n\n\n","category":"function"},{"location":"public_api.html#StochasticAD.perturbations","page":"Public API","title":"StochasticAD.perturbations","text":"perturbations(st::StochasticTriple)\n\nReturn the finite perturbation(s) of st, in a format dependent on the backend used for storing perturbations.\n\n\n\n\n\n","category":"function"},{"location":"public_api.html","page":"Public API","title":"Public API","text":"Note that derivative_estimate is simply the composition of stochastic_triple and derivative_contribution. ","category":"page"},{"location":"public_api.html#Smoothing","page":"Public API","title":"Smoothing","text":"","category":"section"},{"location":"public_api.html","page":"Public API","title":"Public API","text":"What happens if we run derivative_contribution after each step, instead of only at the end? This is smoothing, which combines the second and third components of a single stochastic triple into a single dual component. Smoothing no longer has a guarantee of unbiasedness, but is surprisingly accurate in a number of situations. ","category":"page"},{"location":"public_api.html","page":"Public API","title":"Public API","text":"[Smoothing functionality coming soon.]","category":"page"},{"location":"public_api.html#Optimization","page":"Public API","title":"Optimization","text":"","category":"section"},{"location":"public_api.html","page":"Public API","title":"Public API","text":"We also provide utilities to make it easier to get started with forming and training a model via stochastic gradient descent:","category":"page"},{"location":"public_api.html","page":"Public API","title":"Public API","text":"StochasticAD.StochasticModel\nStochasticAD.stochastic_gradient","category":"page"},{"location":"public_api.html#StochasticAD.StochasticModel","page":"Public API","title":"StochasticAD.StochasticModel","text":"StochasticModel(X, p)\n\nCombine stochastic program X with parameter p into  a trainable model using Functors, where p <: AbstractArray. Formulate as a minimization problem, i.e. find p that minimizes mathbbEX(p).\n\n\n\n\n\n","category":"type"},{"location":"public_api.html#StochasticAD.stochastic_gradient","page":"Public API","title":"StochasticAD.stochastic_gradient","text":"stochastic_gradient(m::StochasticModel)\n\nCompute gradient with respect to the trainable parameter p of StochasticModel(X, p).\n\n\n\n\n\n","category":"function"},{"location":"public_api.html","page":"Public API","title":"Public API","text":"These are used in the tutorial on stochastic optimization.","category":"page"},{"location":"tutorials/optimizations.html#Stochastic-optimizations-with-discrete-randomness","page":"Stochastic optimizations with discrete randomness","title":"Stochastic optimizations with discrete randomness","text":"","category":"section"},{"location":"tutorials/optimizations.html","page":"Stochastic optimizations with discrete randomness","title":"Stochastic optimizations with discrete randomness","text":"import Pkg\nPkg.activate(\"../../../tutorials/toy_optimizations\")\nPkg.develop(path=\"../../..\")\nPkg.instantiate()","category":"page"},{"location":"tutorials/optimizations.html","page":"Stochastic optimizations with discrete randomness","title":"Stochastic optimizations with discrete randomness","text":"In this tutorial, we solve two stochastic optimization problems using StochasticAD where the optimization objective is formed using discrete distributions. We will need the following packages:","category":"page"},{"location":"tutorials/optimizations.html","page":"Stochastic optimizations with discrete randomness","title":"Stochastic optimizations with discrete randomness","text":"using Distributions # defines several supported discrete distributions \nusing StochasticAD\nusing CairoMakie # for plotting\nusing Optimisers # for stochastic gradient descent","category":"page"},{"location":"tutorials/optimizations.html#Optimizing-our-toy-program","page":"Stochastic optimizations with discrete randomness","title":"Optimizing our toy program","text":"","category":"section"},{"location":"tutorials/optimizations.html","page":"Stochastic optimizations with discrete randomness","title":"Stochastic optimizations with discrete randomness","text":"Recall the \"crazy\" program from the intro:","category":"page"},{"location":"tutorials/optimizations.html","page":"Stochastic optimizations with discrete randomness","title":"Stochastic optimizations with discrete randomness","text":"function X(p)\n    a = p * (1 - p)\n    b = rand(Binomial(10, p))\n    c = 2 * b + 3 * rand(Bernoulli(p))\n    return a * c * rand(Normal(b, a))\nend","category":"page"},{"location":"tutorials/optimizations.html","page":"Stochastic optimizations with discrete randomness","title":"Stochastic optimizations with discrete randomness","text":"Let's maximize mathbbEX(p)! First, let's setup the problem, using the StochasticModel helper utility to create a trainable model:","category":"page"},{"location":"tutorials/optimizations.html","page":"Stochastic optimizations with discrete randomness","title":"Stochastic optimizations with discrete randomness","text":"p0 = [0.5] # initial value of p, wrapped in an array for use in the stochastic model\nm = StochasticModel(p -> -X(p[1]), p0) # formulate as minimization problem","category":"page"},{"location":"tutorials/optimizations.html","page":"Stochastic optimizations with discrete randomness","title":"Stochastic optimizations with discrete randomness","text":"Now, let's perform stochastic gradient descent using Adam, where we use stochastic_gradient to obtain a gradient of the model.","category":"page"},{"location":"tutorials/optimizations.html","page":"Stochastic optimizations with discrete randomness","title":"Stochastic optimizations with discrete randomness","text":"iterations = 1000\ntrace = Float64[]\no = Adam() # use Adam for optimization\ns = Optimisers.setup(o, m)\nfor i in 1:iterations\n    # Perform a gradient step\n    Optimisers.update!(s, m, stochastic_gradient(m))\n    push!(trace, m.p[])\nend\np_opt = m.p[] # Our optimized value of p","category":"page"},{"location":"tutorials/optimizations.html","page":"Stochastic optimizations with discrete randomness","title":"Stochastic optimizations with discrete randomness","text":"Finally, let's plot the results of our optimization, and also perform a sweep through the parameter space to verify the accuracy of our estimator:","category":"page"},{"location":"tutorials/optimizations.html","page":"Stochastic optimizations with discrete randomness","title":"Stochastic optimizations with discrete randomness","text":"## Sweep through parameters to find average and derivative\nps = 0.02:0.02:0.98 # values of p to sweep\nN = 1000 # number of samples at each p\navg = [mean(X(p) for _ in 1:N) for p in ps]\nderivative = [mean(derivative_estimate(X, p) for _ in 1:N) for p in ps]\n\n## Make plots\nf = Figure()\nax = f[1, 1] = Axis(f, title = \"Estimates\", xlabel=\"Value of p\")\nlines!(ax, ps, avg, label = \"‚âà E[X(p)]\")\nlines!(ax, ps, derivative, label = \"‚âà d/dp E[X(p)]\")\nvlines!(ax, [p_opt], label = \"p_opt\", color = :green, linewidth = 2.0)\nhlines!(ax, [0.0], color = :black, linewidth = 1.0)\nylims!(ax, (-50, 80))\n\nf[1, 2] = Legend(f, ax, framevisible = false)\nax = f[2, 1:2] = Axis(f, title = \"Optimizer trace\", xlabel=\"Iterations\", ylabel=\"Value of p\")\nlines!(ax, trace, color = :green, linewidth = 2.0)\nsave(\"crazy_opt.png\", f,  px_per_unit = 4) # hide\nnothing # hide","category":"page"},{"location":"tutorials/optimizations.html","page":"Stochastic optimizations with discrete randomness","title":"Stochastic optimizations with discrete randomness","text":"(Image: )","category":"page"},{"location":"tutorials/optimizations.html#Solving-a-variational-problem","page":"Stochastic optimizations with discrete randomness","title":"Solving a variational problem","text":"","category":"section"},{"location":"tutorials/optimizations.html","page":"Stochastic optimizations with discrete randomness","title":"Stochastic optimizations with discrete randomness","text":"Let's consider a toy variational program: we find a Poisson distribution that is close to the distribution of a negative Binomial, via minimization of the Kullback-Leibler divergence D_mathrmKL. Concretely, let us solve","category":"page"},{"location":"tutorials/optimizations.html","page":"Stochastic optimizations with discrete randomness","title":"Stochastic optimizations with discrete randomness","text":"undersetp in mathbbRoperatornameargmin D_mathrmKLleft(mathrmPois(p) hspace3emmiddlehspace3em mathrmNBin(10 025) right)","category":"page"},{"location":"tutorials/optimizations.html","page":"Stochastic optimizations with discrete randomness","title":"Stochastic optimizations with discrete randomness","text":"The following program produces an unbiased estimate of the objective:","category":"page"},{"location":"tutorials/optimizations.html","page":"Stochastic optimizations with discrete randomness","title":"Stochastic optimizations with discrete randomness","text":"function X(p)\n    i = rand(Poisson(p))\n    return logpdf(Poisson(p), i) - logpdf(NegativeBinomial(10, 0.25), i)\nend","category":"page"},{"location":"tutorials/optimizations.html","page":"Stochastic optimizations with discrete randomness","title":"Stochastic optimizations with discrete randomness","text":"We can now optimize the KL-divergence via stochastic gradient descent!","category":"page"},{"location":"tutorials/optimizations.html","page":"Stochastic optimizations with discrete randomness","title":"Stochastic optimizations with discrete randomness","text":"# Minimize E[X] = KL(Poisson(p)| NegativeBinomial(10, 0.25))\niterations = 1000\np0 = [10.0]\nm = StochasticModel(p -> X(p[1]), p0)\ntrace = Float64[]\no = Adam(0.1)\ns = Optimisers.setup(o, m)\nfor i in 1:iterations\n    Optimisers.update!(s, m, stochastic_gradient(m))\n    push!(trace, m.p[])\nend\np_opt = m.p[]","category":"page"},{"location":"tutorials/optimizations.html","page":"Stochastic optimizations with discrete randomness","title":"Stochastic optimizations with discrete randomness","text":"Let's plot our results in the same way as before:","category":"page"},{"location":"tutorials/optimizations.html","page":"Stochastic optimizations with discrete randomness","title":"Stochastic optimizations with discrete randomness","text":"ps = 10:0.5:50\nN = 1000\navg = [mean(X(p) for _ in 1:N) for p in ps]\nderivative = [mean(derivative_estimate(X, p) for _ in 1:N) for p in ps]\nf = Figure()\nax = f[1, 1] = Axis(f, title = \"Estimates\", xlabel=\"Value of p\")\nlines!(ax, ps, avg, label = \"‚âà E[X(p)]\")\nlines!(ax, ps, derivative, label = \"‚âà d/dp E[X(p)]\")\nvlines!(ax, [p_opt], label = \"p_opt\", color = :green, linewidth = 2.0)\nhlines!(ax, [0.0], color = :black, linewidth = 1.0)\nylims!(ax, (-2.5, 5))\n\nf[1, 2] = Legend(f, ax, framevisible = false)\nax = f[2, 1:2] = Axis(f, title = \"Optimizer trace\", ylabel=\"Value of p\", xlabel=\"Iterations\")\nlines!(ax, trace, color = :green, linewidth = 2.0)\nsave(\"variational.png\", f, px_per_unit = 4) # hide\nnothing # hide","category":"page"},{"location":"tutorials/optimizations.html","page":"Stochastic optimizations with discrete randomness","title":"Stochastic optimizations with discrete randomness","text":"(Image: )","category":"page"},{"location":"tutorials/particle_filter.html#Differentiable-particle-filter","page":"Differentiable particle filter","title":"Differentiable particle filter","text":"","category":"section"},{"location":"tutorials/particle_filter.html","page":"Differentiable particle filter","title":"Differentiable particle filter","text":"Using a bootstrap particle sampler, we can approximate the posterior distributions of the states given noisy and partial observations of the state of a hidden Markov model by a cloud of K weighted particles with weights W.","category":"page"},{"location":"tutorials/particle_filter.html","page":"Differentiable particle filter","title":"Differentiable particle filter","text":"In this tutorial, we are going to:","category":"page"},{"location":"tutorials/particle_filter.html","page":"Differentiable particle filter","title":"Differentiable particle filter","text":"implement a differentiable particle filter based on StochasticAD.jl.\nvisualize the particle filter in d = 2 dimensions.\ncompare the gradient based on the differentiable particle filter to a biased gradient estimator as well as to the gradient of a differentiable Kalman filter.\nshow how to benchmark primal evaluation, forward- and reverse-mode AD of the particle filter.","category":"page"},{"location":"tutorials/particle_filter.html#Setup","page":"Differentiable particle filter","title":"Setup","text":"","category":"section"},{"location":"tutorials/particle_filter.html","page":"Differentiable particle filter","title":"Differentiable particle filter","text":"We will make use of several julia packages. For example, we are going to use Distributions and DistributionsAD that implement the reparameterization trick for Gaussian distributions used in the observation and state-transition model, which we specify below. We also import GaussianDistributions.jl to implement the differentiable Kalman filter.","category":"page"},{"location":"tutorials/particle_filter.html#Package-dependencies","page":"Differentiable particle filter","title":"Package dependencies","text":"","category":"section"},{"location":"tutorials/particle_filter.html","page":"Differentiable particle filter","title":"Differentiable particle filter","text":"import Pkg\nPkg.activate(\"../../../tutorials\")\nPkg.develop(path=\"../../..\")\nPkg.instantiate()","category":"page"},{"location":"tutorials/particle_filter.html","page":"Differentiable particle filter","title":"Differentiable particle filter","text":"# activate tutorial project file\n\n# load dependencies\nusing StochasticAD\nusing Distributions\nusing DistributionsAD\nusing Random\nusing Statistics\nusing StatsBase\nusing LinearAlgebra\nusing Zygote\nusing ForwardDiff\nusing GaussianDistributions\nusing GaussianDistributions: correct, ‚äï\nusing Measurements\nusing UnPack\nusing Plots\nusing LaTeXStrings\nusing BenchmarkTools","category":"page"},{"location":"tutorials/particle_filter.html#Particle-filter","page":"Differentiable particle filter","title":"Particle filter","text":"","category":"section"},{"location":"tutorials/particle_filter.html","page":"Differentiable particle filter","title":"Differentiable particle filter","text":"For convenience, we first introduce the new type StochasticModel with the following fields:","category":"page"},{"location":"tutorials/particle_filter.html","page":"Differentiable particle filter","title":"Differentiable particle filter","text":"T: total number of time steps.\nstart: starting distribution for the initial state. For example, in the form of a narrow  Gaussian start(Œ∏) = Gaussian(x0, 0.001 * I(d)).\ndyn: pointwise differentiable stochastic program in the form of Markov transition densities.  For example, dyn(x, Œ∏) = MvNormal(reshape(Œ∏, d, d) * x, Q(Œ∏)), where Q(Œ∏) denotes the  covariance matrix.\nobs: observation model having a smooth conditional probability density depending on  current state x and parameters Œ∏. For example, obs(x, Œ∏) = MvNormal(x, R(Œ∏)),  where R(Œ∏) denotes the covariance matrix.","category":"page"},{"location":"tutorials/particle_filter.html","page":"Differentiable particle filter","title":"Differentiable particle filter","text":"For parameters Œ∏,  rand(start(Œ∏)) gives a sample from the prior distribution of the starting distribution. For current state x and parameters Œ∏, xnew = rand(dyn(x, Œ∏)) samples the new state (i.e. dyn gives for each x, Œ∏ a distribution-like object). Finally, y = rand(obs(x, Œ∏)) samples an observation.","category":"page"},{"location":"tutorials/particle_filter.html","page":"Differentiable particle filter","title":"Differentiable particle filter","text":"We can then define the ParticleFilter type that wraps a stochastic model StochM::StochasticModel, a sampling strategy (with arguments p, K, sump=1) and observational data ys. For simplicity, our implementation assumes a observation-likelihood function being available via pdf(obs(x, Œ∏), y).","category":"page"},{"location":"tutorials/particle_filter.html","page":"Differentiable particle filter","title":"Differentiable particle filter","text":"struct StochasticModel{TType<:Integer,T1,T2,T3}\n    T::TType # time steps\n    start::T1 # prior\n    dyn::T2 # dynamical model\n    obs::T3 # observation model\nend\n\nstruct ParticleFilter{mType<:Integer,MType<:StochasticModel,yType,sType}\n    m::mType # number of particles\n    StochM::MType # stochastic model\n    ys::yType # observations\n    sample_strategy::sType # sampling function\nend","category":"page"},{"location":"tutorials/particle_filter.html#Kalman-filter","page":"Differentiable particle filter","title":"Kalman filter","text":"","category":"section"},{"location":"tutorials/particle_filter.html","page":"Differentiable particle filter","title":"Differentiable particle filter","text":"We consider a stochastic program that fulfills the assumptions of a Kalman filter. We follow Kalman.jl to implement a differentiable version. Our KalmanFilter type wraps a stochastic model StochM::StochasticModel and observational data ys. It assumes a observation-likelihood function is implemented via llikelihood(yres, S). The Kalman filter contains the following fields:","category":"page"},{"location":"tutorials/particle_filter.html","page":"Differentiable particle filter","title":"Differentiable particle filter","text":"d: dimension of the state-transition matrix Phi according to x = Phi x + w with w sim operatornameNormal(0Q).\nStochM: Stochastic model of type StochasticModel.\nH: linear map from the state space into the observed space according to y = H x + nu with nu sim operatornameNormal(0R).\nR: covariance matrix entering the observation model according to y = H x + nu with nu sim operatornameNormal(0R).\nQ: covariance matrix entering the state-transition model according to x = Phi x + w with w sim operatornameNormal(0Q).\nys: observations.","category":"page"},{"location":"tutorials/particle_filter.html","page":"Differentiable particle filter","title":"Differentiable particle filter","text":"llikelihood(yres, S) = GaussianDistributions.logpdf(Gaussian(zero(yres), Symmetric(S)), yres)\nstruct KalmanFilter{dType<:Integer,MType<:StochasticModel,HType,RType,QType,yType}\n    # H, R = obs\n    # Œ∏, Q = dyn\n    d::dType\n    StochM::MType # stochastic model\n    H::HType # observation model, maps the true state space into the observed space\n    R::RType # observation model, covariance matrix\n    Q::QType # dynamical model, covariance matrix\n    ys::yType # observations\nend","category":"page"},{"location":"tutorials/particle_filter.html","page":"Differentiable particle filter","title":"Differentiable particle filter","text":"To get observations ys from the latent states xs based on the (true, potentially unknown) parameters Œ∏, we simulate a single particle from the forward model returning a vector of observations (no resampling steps).","category":"page"},{"location":"tutorials/particle_filter.html","page":"Differentiable particle filter","title":"Differentiable particle filter","text":"function simulate_single(StochM::StochasticModel, Œ∏)\n    @unpack T, start, dyn, obs = StochM\n    x = rand(start(Œ∏))\n    y = rand(obs(x, Œ∏))\n    xs = [x]\n    ys = [y]\n    for t in 2:T\n        x = rand(dyn(x, Œ∏))\n        y = rand(obs(x, Œ∏))\n        push!(xs, x)\n        push!(ys, y)\n    end\n    xs, ys\nend","category":"page"},{"location":"tutorials/particle_filter.html","page":"Differentiable particle filter","title":"Differentiable particle filter","text":"A particle filter becomes efficient if resampling steps are included. Resampling is numerically attractive because particles with small weight are discarded, so computational resources are not wasted on particles with vanishing weight.","category":"page"},{"location":"tutorials/particle_filter.html","page":"Differentiable particle filter","title":"Differentiable particle filter","text":"Here, let us implement a stratified resampling strategy, see for example Murray (2012), where p denotes the probabilities of K particles with sump = sum(p).","category":"page"},{"location":"tutorials/particle_filter.html","page":"Differentiable particle filter","title":"Differentiable particle filter","text":"function sample_stratified(p, K, sump=1)\n    n = length(p)\n    U = rand()\n    is = zeros(Int, K)\n    i = 1\n    cw = p[1]\n    for k in 1:K\n        t = sump * (k - 1 + U) / K\n        while cw < t && i < n\n            i += 1\n            @inbounds cw += p[i]\n        end\n        is[k] = i\n    end\n    return is\nend","category":"page"},{"location":"tutorials/particle_filter.html","page":"Differentiable particle filter","title":"Differentiable particle filter","text":"This sampling strategy can be used within a differentiable resampling step in our particle filter using the use_new_weight function as implemented in StochasticAD.jl. The resample function below returns the states X_new and weights W_new of the resampled particles.","category":"page"},{"location":"tutorials/particle_filter.html","page":"Differentiable particle filter","title":"Differentiable particle filter","text":"m: number of particles.\nX: current particle states.\nW: current weight vector of the particles.\nœâ == sum(W) is an invariant.\nsample_strategy: specific resampling strategy to be used. For example, sample_stratified.\nuse_new_weight=true: Allows one to switch between biased, stop-gradient method and  differentiable resampling step.","category":"page"},{"location":"tutorials/particle_filter.html","page":"Differentiable particle filter","title":"Differentiable particle filter","text":"function resample(m, X, W, œâ, sample_strategy, use_new_weight=true)\n    js = Zygote.ignore(() -> sample_strategy(W, m, œâ))\n    X_new = X[js]\n    if use_new_weight\n        # differentiable resampling\n        W_chosen = W[js]\n        W_new = map(w -> œâ * new_weight(w / œâ) / m, W_chosen)\n    else\n        # stop gradient, biased approach\n        W_new = fill(œâ / m, m)\n    end\n    X_new, W_new\nend","category":"page"},{"location":"tutorials/particle_filter.html","page":"Differentiable particle filter","title":"Differentiable particle filter","text":"Note that we added a if condition that allows us to switch between the differentiable resampling step and the stop-gradient approach.","category":"page"},{"location":"tutorials/particle_filter.html","page":"Differentiable particle filter","title":"Differentiable particle filter","text":"We're now equipped with all primitive operations to set up the particle filter, which propagates particles with weights W preserving the invariant œâ == sum(W). We never normalize W and, therefore, œâ in the code below contains likelihood information. The particle-filter implementation defaults to return particle positions and weights at T if store_path=false and takes the following input arguments:","category":"page"},{"location":"tutorials/particle_filter.html","page":"Differentiable particle filter","title":"Differentiable particle filter","text":"Œ∏: parameters for the stochastic program (state-transition and observation model).\nstore_path=false: Option to store the path of the particles, e.g. to visualize/inspect their trajectories.\nuse_new_weight=true: Option to switch between the stop-gradient and our differentiable resampling step method. Defaults to using differentiable resampling.\ns: controls the number of resampling steps according to t > 1 && t < T && (t % s == 0).","category":"page"},{"location":"tutorials/particle_filter.html","page":"Differentiable particle filter","title":"Differentiable particle filter","text":"function (F::ParticleFilter)(Œ∏; store_path=false, use_new_weight=true, s=1)\n    # s controls the number of resampling steps\n    @unpack m, StochM, ys, sample_strategy = F\n    @unpack T, start, dyn, obs = StochM\n\n\n    X = [rand(start(Œ∏)) for j in 1:m] # particles\n    W = [1 / m for i in 1:m] # weights\n    œâ = 1 # total weight\n    store_path && (Xs = [X])\n    for (t, y) in zip(1:T, ys)\n        # update weights & likelihood using observations\n        wi = map(x -> pdf(obs(x, Œ∏), y), X)\n        W = W .* wi\n        œâ_old = œâ\n        œâ = sum(W)\n        # resample particles\n        if t > 1 && t < T && (t % s == 0) # && 1 / sum((W / œâ) .^ 2) < length(W) √∑ 32\n            X, W = resample(m, X, W, œâ, sample_strategy, use_new_weight)\n        end\n        # update particle states\n        if t < T\n            X = map(x -> rand(dyn(x, Œ∏)), X)\n            store_path && Zygote.ignore(() -> push!(Xs, X))\n        end\n    end\n    (store_path ? Xs : X), W\nend","category":"page"},{"location":"tutorials/particle_filter.html","page":"Differentiable particle filter","title":"Differentiable particle filter","text":"Following Kalman.jl, we implement a differentiable Kalman filter to check the ground-truth gradient. Our Kalman filter returns an updated posterior state estimate and the log-likelihood and takes the parameters of the stochastic program as an input.","category":"page"},{"location":"tutorials/particle_filter.html","page":"Differentiable particle filter","title":"Differentiable particle filter","text":"function (F::KalmanFilter)(Œ∏)\n    @unpack d, StochM, H, R, Q = F\n    @unpack start = StochM\n\n    x = start(Œ∏)\n    Œ¶ = reshape(Œ∏, d, d)\n\n    x, yres, S = GaussianDistributions.correct(x, ys[1] + R, H)\n    ll = llikelihood(yres, S)\n    xs = Any[x]\n    for i in 2:length(ys)\n        x = Œ¶ * x ‚äï Q\n        x, yres, S = GaussianDistributions.correct(x, ys[i] + R, H)\n        ll += llikelihood(yres, S)\n\n        push!(xs, x)\n    end\n    xs, ll\nend","category":"page"},{"location":"tutorials/particle_filter.html","page":"Differentiable particle filter","title":"Differentiable particle filter","text":"For both filters, it is straightforward to obtain the log-likelihood via:","category":"page"},{"location":"tutorials/particle_filter.html","page":"Differentiable particle filter","title":"Differentiable particle filter","text":"function log_likelihood(F::ParticleFilter, Œ∏, use_new_weight=true, s=1)\n    _, W = F(Œ∏; store_path=false, use_new_weight=use_new_weight, s=s)\n    log(sum(W))\nend","category":"page"},{"location":"tutorials/particle_filter.html","page":"Differentiable particle filter","title":"Differentiable particle filter","text":"and","category":"page"},{"location":"tutorials/particle_filter.html","page":"Differentiable particle filter","title":"Differentiable particle filter","text":"function log_likelihood(F::KalmanFilter, Œ∏)\n    _, ll = F(Œ∏)\n    ll\nend","category":"page"},{"location":"tutorials/particle_filter.html","page":"Differentiable particle filter","title":"Differentiable particle filter","text":"For convenience, we define functions for","category":"page"},{"location":"tutorials/particle_filter.html","page":"Differentiable particle filter","title":"Differentiable particle filter","text":"forward-mode AD (and differentiable resampling step) to compute the gradient of the log-likelihood of the particle filter.\nreverse-mode AD (and differentiable resampling step) to compute the gradient of the log-likelihood of the particle filter.\nforward-mode AD (and stop-gradient method) to compute the gradient of the log-likelihood of the particle filter (without the new_weight function).\nforward-mode AD to compute the gradient of the log-likelihood of the Kalman filter.","category":"page"},{"location":"tutorials/particle_filter.html","page":"Differentiable particle filter","title":"Differentiable particle filter","text":"\nforw_grad(Œ∏, F::ParticleFilter; s=1) = ForwardDiff.gradient(Œ∏ -> log_likelihood(F, Œ∏, true, s), Œ∏)\nback_grad(Œ∏, F::ParticleFilter; s=1) = Zygote.gradient(Œ∏ -> log_likelihood(F, Œ∏, true, s), Œ∏)[1]\nforw_grad_biased(Œ∏, F::ParticleFilter; s=1) = ForwardDiff.gradient(Œ∏ -> log_likelihood(F, Œ∏, false, s), Œ∏)\nforw_grad_Kalman(Œ∏, F::KalmanFilter) = ForwardDiff.gradient(Œ∏ -> log_likelihood(F, Œ∏), Œ∏)","category":"page"},{"location":"tutorials/particle_filter.html#Model","page":"Differentiable particle filter","title":"Model","text":"","category":"section"},{"location":"tutorials/particle_filter.html","page":"Differentiable particle filter","title":"Differentiable particle filter","text":"Having set up all core functionalities, we can now define the specific stochastic model.","category":"page"},{"location":"tutorials/particle_filter.html","page":"Differentiable particle filter","title":"Differentiable particle filter","text":"We consider the following system with a d-dimensional latent process,","category":"page"},{"location":"tutorials/particle_filter.html","page":"Differentiable particle filter","title":"Differentiable particle filter","text":"beginaligned\nx_i = Phi x_i-1 + w_i text with  w_i sim operatornameNormal(0Q)\ny_i = x_i + nu_i text with  nu_i sim operatornameNormal(0R)\nendaligned","category":"page"},{"location":"tutorials/particle_filter.html","page":"Differentiable particle filter","title":"Differentiable particle filter","text":"where Phi is a d-dimensional rotation matrix.","category":"page"},{"location":"tutorials/particle_filter.html","page":"Differentiable particle filter","title":"Differentiable particle filter","text":"seed = 423897\n\n### Define model\n# here: n-dimensional rotation matrix\nRandom.seed!(seed)\nT = 20 # time steps\nd = 2 # dimension\n# generate a rotation matrix\nM = randn(d, d)\nc = 0.3 # scaling\nO = exp(c * (M - transpose(M)) / 2)\n@assert det(O) ‚âà 1\n@assert transpose(O) * O ‚âà I(d)\nŒ∏true = vec(O) # true parameter\n\n# observation model\nR = 0.01 * collect(I(d))\nobs(x, Œ∏) = MvNormal(x, R) # y = H x + ŒΩ with ŒΩ ~ Normal(0, R)\n\n# dynamical model\nQ = 0.02 * collect(I(d))\ndyn(x, Œ∏) = MvNormal(reshape(Œ∏, d, d) * x, Q) #  x = Œ¶*x + w with w ~ Normal(0,Q)\n\n# starting position\nx0 = randn(d)\n# prior distribution\nstart(Œ∏) = Gaussian(x0, 0.001 * collect(I(d)))\n\n# put it all together\nstochastic_model = StochasticModel(T, start, dyn, obs)\n\n# relevant corresponding Kalman filterng defs\nH_Kalman = collect(I(d))\nR_Kalman = Gaussian(zeros(Float64, d), R)\n# Œ¶_Kalman = O\nQ_Kalman = Gaussian(zeros(Float64, d), Q)\n###\n\n### simulate model\nRandom.seed!(seed)\nxs, ys = simulate_single(stochastic_model, Œ∏true)","category":"page"},{"location":"tutorials/particle_filter.html#Visualization","page":"Differentiable particle filter","title":"Visualization","text":"","category":"section"},{"location":"tutorials/particle_filter.html","page":"Differentiable particle filter","title":"Differentiable particle filter","text":"Using particle_filter(Œ∏; store_path=true) and kalman_filter(Œ∏), it is straightforward to visualize both filters for our observed data.","category":"page"},{"location":"tutorials/particle_filter.html","page":"Differentiable particle filter","title":"Differentiable particle filter","text":"m = 1000\nkalman_filter = KalmanFilter(d, stochastic_model, H_Kalman, R_Kalman, Q_Kalman, ys)\nparticle_filter = ParticleFilter(m, stochastic_model, ys, sample_stratified)","category":"page"},{"location":"tutorials/particle_filter.html","page":"Differentiable particle filter","title":"Differentiable particle filter","text":"### run and visualize filters\nXs, W = particle_filter(Œ∏true; store_path=true)\nfig = plot(getindex.(xs, 1), getindex.(xs, 2), legend=false, xlabel=L\"x_1\", ylabel=L\"x_2\") # x1 and x2 are bad names..conflicting notation\nscatter!(fig, getindex.(ys, 1), getindex.(ys, 2))\nfor i in 1:min(m, 100) # note that Xs has obs noise.\n    local xs = [Xs[t][i] for t in 1:T]\n    scatter!(fig, getindex.(xs, 1), getindex.(xs, 2), marker_z=1:T, color=:cool, alpha=0.1) # color to indicate time step\nend\n\nxs_Kalman, ll_Kalman = kalman_filter(Œ∏true)\nplot!(getindex.(mean.(xs_Kalman), 1), getindex.(mean.(xs_Kalman), 2), legend=false, color=\"red\")\npng(\"pf_1\") # hide","category":"page"},{"location":"tutorials/particle_filter.html","page":"Differentiable particle filter","title":"Differentiable particle filter","text":"(Image: )","category":"page"},{"location":"tutorials/particle_filter.html#Bias","page":"Differentiable particle filter","title":"Bias","text":"","category":"section"},{"location":"tutorials/particle_filter.html","page":"Differentiable particle filter","title":"Differentiable particle filter","text":"We can also investigate the distribution of the gradients from the particle filter with and without differentiable resampling step, as compared to the gradient computed by differentiating the Kalman filter.","category":"page"},{"location":"tutorials/particle_filter.html","page":"Differentiable particle filter","title":"Differentiable particle filter","text":"### compute gradients\nRandom.seed!(seed)\nX = [forw_grad(Œ∏true, particle_filter) for i in 1:200] # gradient of the particle filter *with* differentiation of the resampling step\nRandom.seed!(seed)\nXbiased = [forw_grad_biased(Œ∏true, particle_filter) for i in 1:200] # Gradient of the particle filter *without* differentiation of the resampling step\n# pick an arbitrary coordinate\nindex = 1 # take derivative with respect to first parameter (2-dimensional example has a rotation matrix with four parameters in total)\n# plot histograms for the sampled derivative values\nfig = plot(normalize(fit(Histogram, getindex.(X, index), nbins=20), mode=:pdf), legend=false) # ours\nplot!(normalize(fit(Histogram, getindex.(Xbiased, index), nbins=20), mode=:pdf)) # biased\nvline!([mean(X)[index]], color=1)\nvline!([mean(Xbiased)[index]], color=2)\n# add derivative of differentiable Kalman filter as a comparison\nXK = forw_grad_Kalman(Œ∏true, kalman_filter)\nvline!([XK[index]], color=\"black\")\npng(\"pf_2\") # hide","category":"page"},{"location":"tutorials/particle_filter.html","page":"Differentiable particle filter","title":"Differentiable particle filter","text":"(Image: )","category":"page"},{"location":"tutorials/particle_filter.html","page":"Differentiable particle filter","title":"Differentiable particle filter","text":"The estimator using the new_weight function agrees with the gradient value from the Kalman filter and the particle filter AD scheme developed by ≈öcibior and Wood, unlike biased estimators that neglect the contribution of the derivative from the resampling step. However, the biased estimator displays a smaller variance.","category":"page"},{"location":"tutorials/particle_filter.html#Benchmark","page":"Differentiable particle filter","title":"Benchmark","text":"","category":"section"},{"location":"tutorials/particle_filter.html","page":"Differentiable particle filter","title":"Differentiable particle filter","text":"Finally, we can use BenchmarkTools.jl to benchmark the run times of the primal pass with respect to forward-mode and reverse-mode AD of the particle filter. As expected, forward-mode AD outperforms reverse-mode AD for the small number of parameters considered here.","category":"page"},{"location":"tutorials/particle_filter.html","page":"Differentiable particle filter","title":"Differentiable particle filter","text":"# secs for how long the benchmark should run, see https://juliaci.github.io/BenchmarkTools.jl/stable/\nsecs = 1\n\nsuite = BenchmarkGroup()\nsuite[\"scaling\"] = BenchmarkGroup([\"grads\"])\n\nsuite[\"scaling\"][\"primal\"] = @benchmarkable log_likelihood(particle_filter, Œ∏true)\nsuite[\"scaling\"][\"forward\"] = @benchmarkable forw_grad(Œ∏true, particle_filter)\nsuite[\"scaling\"][\"backward\"] = @benchmarkable back_grad(Œ∏true, particle_filter)\n\ntune!(suite)\nresults = run(suite, verbose=true, seconds=secs)\n\nt1 = measurement(mean(results[\"scaling\"][\"primal\"].times), std(results[\"scaling\"][\"primal\"].times) / sqrt(length(results[\"scaling\"][\"primal\"].times)))\nt2 = measurement(mean(results[\"scaling\"][\"forward\"].times), std(results[\"scaling\"][\"forward\"].times) / sqrt(length(results[\"scaling\"][\"forward\"].times)))\nt3 = measurement(mean(results[\"scaling\"][\"backward\"].times), std(results[\"scaling\"][\"backward\"].times) / sqrt(length(results[\"scaling\"][\"backward\"].times)))\n@show t1 t2 t3\n\nts = (t1, t2, t3) ./ 10^6 # ms\n@show ts","category":"page"},{"location":"tutorials/game_of_life.html#Stochastic-Game-of-Life","page":"Stochastic Game of Life","title":"Stochastic Game of Life","text":"","category":"section"},{"location":"tutorials/game_of_life.html","page":"Stochastic Game of Life","title":"Stochastic Game of Life","text":"We consider a stochastic version of Conway's Game of Life, played on a two-dimensional board. We shall use the following packages,","category":"page"},{"location":"tutorials/game_of_life.html","page":"Stochastic Game of Life","title":"Stochastic Game of Life","text":"import Pkg\nPkg.activate(\"../../../tutorials\")\nPkg.develop(path=\"../../..\")\nPkg.instantiate()","category":"page"},{"location":"tutorials/game_of_life.html","page":"Stochastic Game of Life","title":"Stochastic Game of Life","text":"using Distributions\nusing StochasticAD\nusing OffsetArrays \nusing StaticArrays","category":"page"},{"location":"tutorials/game_of_life.html#Setting-up-the-stochastic-Game-of-Life","page":"Stochastic Game of Life","title":"Setting up the stochastic Game of Life","text":"","category":"section"},{"location":"tutorials/game_of_life.html","page":"Stochastic Game of Life","title":"Stochastic Game of Life","text":"Each turn, the standard Game of Life applies the following rules to each cell,","category":"page"},{"location":"tutorials/game_of_life.html","page":"Stochastic Game of Life","title":"Stochastic Game of Life","text":"textdead and 3 neighbours alive to text alive \ntextalive and 0 1 or 4 neighbours alive to text dead","category":"page"},{"location":"tutorials/game_of_life.html","page":"Stochastic Game of Life","title":"Stochastic Game of Life","text":"The cell's status does not change otherwise. In our stochastic version, these rules instead occur with probability 1-Œ∏, while the opposite event has probability Œ∏. To initialize the board at the beginning of the game, we randomly set each cell alive with probability p. ","category":"page"},{"location":"tutorials/game_of_life.html","page":"Stochastic Game of Life","title":"Stochastic Game of Life","text":"The following high level function sets up the probabilities and provides them to play_game_of_life.","category":"page"},{"location":"tutorials/game_of_life.html","page":"Stochastic Game of Life","title":"Stochastic Game of Life","text":"function play(p, Œ∏=0.1, N=12, T=10; log=false)\n    # N is the board half-length, T are game time steps\n    low = Œ∏\n    high = 1-Œ∏\n    birth_probs = SA[low, low, low, high, low] # 0, 1, 2, 3, 4 neighbours\n    death_probs = SA[high, high, low, low, high] # 0, 1, 2, 3, 4 neighbours \n    return play_game_of_life(p, vcat(birth_probs, death_probs), N, T; log)\nend","category":"page"},{"location":"tutorials/game_of_life.html","page":"Stochastic Game of Life","title":"Stochastic Game of Life","text":"We can now implement the Game of Life based on the specification. At the end of the game, we return the total number of alive cells.","category":"page"},{"location":"tutorials/game_of_life.html","page":"Stochastic Game of Life","title":"Stochastic Game of Life","text":"# A single turn of the game\nfunction update_state(all_probs, N, board_new, board_old)\n    for i in -N:N\n        for j in -N:N\n            neighbours = board_old[i+1, j] + board_old[i-1, j] + board_old[i, j-1] + board_old[i, j+1]\n            index = board_new[i,j] * 5 + neighbours + 1 \n            b = rand(Bernoulli(all_probs[index]))\n            board_new[i,j] += (1 - 2 * board_new[i,j]) * b \n        end\n    end\nend\n\nfunction play_game_of_life(p, all_probs, N, T; log=false)\n    dual_type = promote_type(typeof(rand(Bernoulli(p))), typeof.(rand.(Bernoulli.(all_probs)))...) # a hacky way of getting the correct array type \n    board = OffsetArray(zeros(dual_type, 2*N + 3, 2*N + 3), -(N+1):(N+1), -(N+1):(N+1)) # center board at (0,0), pad by 1 \n\n    # initialize the board\t\n    for i in -N:N\n        for j in -N:N\n            board[i,j] = rand(Bernoulli(p))\n        end\n    end\n    board_old = similar(board)\n    log && (history = [])\n\n    # play the game\n    for time_step in 1:T\n        copy!(board_old, board)\n        update_state(all_probs, N, board, board_old)\n        log && push!(history, copy(board))\n    end\n\n    if !log\n        return sum(board)\n    else\n        return sum(board), board, history\n    end\nend\n\nplay(0.5, 0.1) # play the game with p = 0.5 and Œ∏ = 0.1","category":"page"},{"location":"tutorials/game_of_life.html","page":"Stochastic Game of Life","title":"Stochastic Game of Life","text":"note: Note\nNote that we did have to be careful to write this program to be compatible with the current capabilities of StochasticAD. For example, we concatenated birth_probs and death_probs into a single array all_probs and used the index board[i, j] * 5 + neighbours + 1 to find the probability, rather than use the more natural if alive... else... syntax.","category":"page"},{"location":"tutorials/game_of_life.html#Differentiating-the-Game-of-Life","page":"Stochastic Game of Life","title":"Differentiating the Game of Life","text":"","category":"section"},{"location":"tutorials/game_of_life.html","page":"Stochastic Game of Life","title":"Stochastic Game of Life","text":"Let's differentiate the Game of Life!","category":"page"},{"location":"tutorials/game_of_life.html","page":"Stochastic Game of Life","title":"Stochastic Game of Life","text":"@show stochastic_triple(play, 0.5) # let's take a look at a single stochastic triple\n\nsamples = [derivative_estimate(play, 0.5) for i in 1:10000] # take many samples\nderivative = mean(samples)\nuncertainty = std(samples) / sqrt(10000)\nprintln(\"derivative of ùîº[play(p)] = $derivative ¬± $uncertainty\")","category":"page"},{"location":"tutorials/game_of_life.html","page":"Stochastic Game of Life","title":"Stochastic Game of Life","text":"The following sketch of the final state of the board for a single run gives some insight into what the stochastic triples are doing. The original board is depicted in grey and white for dead and alive, and the cells which flip from dead to alive in the \"alternative\" path consider by the triples are marked with + signs, while the cells which flip from alive to dead are marked with X signs.","category":"page"},{"location":"tutorials/game_of_life.html","page":"Stochastic Game of Life","title":"Stochastic Game of Life","text":"<img src=\"../images/final_gol_board.png\" width=\"50%\"/>","category":"page"},{"location":"tutorials/game_of_life.html","page":"Stochastic Game of Life","title":"Stochastic Game of Life","text":"‚†Ä","category":"page"},{"location":"tutorials/random_walk.html#Random-walk","page":"Random walk","title":"Random walk","text":"","category":"section"},{"location":"tutorials/random_walk.html","page":"Random walk","title":"Random walk","text":"import Pkg\nPkg.activate(\"../../../tutorials\")\nPkg.develop(path=\"../../..\")\nPkg.instantiate()","category":"page"},{"location":"tutorials/random_walk.html","page":"Random walk","title":"Random walk","text":"In this tutorial, we differentiate a random walk over the integers using StochasticAD. We will need the following packages,","category":"page"},{"location":"tutorials/random_walk.html","page":"Random walk","title":"Random walk","text":"using Distributions # defines several supported discrete distributions \nusing StochasticAD\nusing StaticArrays # for more efficient small arrays","category":"page"},{"location":"tutorials/random_walk.html#Setting-up-the-random-walk","page":"Random walk","title":"Setting up the random walk","text":"","category":"section"},{"location":"tutorials/random_walk.html","page":"Random walk","title":"Random walk","text":"Let's define a function for simulating the walk.","category":"page"},{"location":"tutorials/random_walk.html","page":"Random walk","title":"Random walk","text":"function simulate_walk(probs, steps, n)\n    state = 0\n    for i in 1:n\n        probs_here = probs(state) # transition probabilities for possible steps\n        step_index = rand(Categorical(probs_here)) # which step do we take?\n        step = steps[step_index] # get size of step \n        state += step\n    end\n    return state\nend","category":"page"},{"location":"tutorials/random_walk.html","page":"Random walk","title":"Random walk","text":"Here, steps is a (1-indexed) array of the possible steps we can take. Each of these steps has a certain probability. To make things more interesting, we take in a function probs to produce these probabilities that can depend on the current state of the random walk.","category":"page"},{"location":"tutorials/random_walk.html","page":"Random walk","title":"Random walk","text":"Let's zoom in on the two lines where discrete randomness is involved. ","category":"page"},{"location":"tutorials/random_walk.html","page":"Random walk","title":"Random walk","text":"step_index = rand(Categorical(probs_here)) # which step do we take?\nstep = steps[step_index] # get size of step ","category":"page"},{"location":"tutorials/random_walk.html","page":"Random walk","title":"Random walk","text":"This is a cute pattern for making a discrete choice. First, we sample from a Categorical distribution from Distributions.jl, using the probabilities probs_here at our current position. This gives us an index between 1 and length(steps), which we can use to pick the actual step to take. Stochastic triples propagate through both steps!","category":"page"},{"location":"tutorials/random_walk.html#Differentiating-the-random-walk","page":"Random walk","title":"Differentiating the random walk","text":"","category":"section"},{"location":"tutorials/random_walk.html","page":"Random walk","title":"Random walk","text":"Let's define a toy problem. We consider a random walk with -1 and +1 steps, where the probability of +1 starts off high but decays exponentially with a decay length of p. We take n = 100 steps and set p = 50.","category":"page"},{"location":"tutorials/random_walk.html","page":"Random walk","title":"Random walk","text":"using StochasticAD\n\nconst steps = SA[-1, 1] # move left or move right\nmake_probs(p) = X -> SA[1 - exp(-X / p), exp(-X / p)]\n\nf(p, n) = simulate_walk(make_probs(p), steps, n)\n@show f(50, 100) # let's run a single random walk with p = 50\n@show stochastic_triple(p -> f(p, 100), 50) # let's see how a single stochastic triple looks like at p = 50","category":"page"},{"location":"tutorials/random_walk.html","page":"Random walk","title":"Random walk","text":"Time to differentiate! For fun, let's differentiate the square of the output of the random walk.","category":"page"},{"location":"tutorials/random_walk.html","page":"Random walk","title":"Random walk","text":"f_squared(p, n) = f(p, n)^2\n\nsamples = [derivative_estimate(p -> f_squared(p, 100), 50) for i in 1:1000] # many samples from derivative program at p = 50\nderivative = mean(samples)\nuncertainty = std(samples) / sqrt(1000)\nprintln(\"derivative of ùîº[f_squared] = $derivative ¬± $uncertainty\")","category":"page"},{"location":"tutorials/random_walk.html#Computing-variance","page":"Random walk","title":"Computing variance","text":"","category":"section"},{"location":"tutorials/random_walk.html","page":"Random walk","title":"Random walk","text":"A crucial figure of merit for a derivative estimator is its variance. We compute the standard deviation (square root of the variance) of our estimator over a range of n.","category":"page"},{"location":"tutorials/random_walk.html","page":"Random walk","title":"Random walk","text":"n_range = 10:10:100 # range for testing asymptotic variance behaviour\np_range = 2 .* n_range\nnsamples = 10000\n\nstds_triple = Float64[]\nfor (n, p) in zip(n_range, p_range)\n    std_triple = std(derivative_estimate(p -> f_squared(p, n), p)\n                     for i in 1:(nsamples))\n    push!(stds_triple, std_triple)\nend\n@show stds_triple","category":"page"},{"location":"tutorials/random_walk.html","page":"Random walk","title":"Random walk","text":"For comparison with other unbiased estimators, we also compute stds_score and stds_score_baseline for the score function gradient estimator, both without and with a variance-reducing batch-average control variate (CV). (For details, see core.jl and compare_score.jl.) We can now graph the standard deviation of each estimator versus n, observing lower variance in the unbiased derivative estimate produced by stochastic triples:","category":"page"},{"location":"tutorials/random_walk.html","page":"Random walk","title":"Random walk","text":"<img src=\"../images/compare_score.png\" width=\"50%\"/>","category":"page"},{"location":"tutorials/random_walk.html","page":"Random walk","title":"Random walk","text":"‚†Ä","category":"page"},{"location":"limitations.html#Limitations-of-StochasticAD","page":"Limitations","title":"Limitations of StochasticAD","text":"","category":"section"},{"location":"limitations.html","page":"Limitations","title":"Limitations","text":"StochasticAD has a number of limitations that are important to be aware of:","category":"page"},{"location":"limitations.html","page":"Limitations","title":"Limitations","text":"StochasticAD uses operator-overloading just like ForwardDiff, so all of the limitations listed there apply here too. Also note that some useful features of ForwardDiff, such as chunking for greater efficiency with a large number of parameters, have not yet been implemented here.\nWe have limited support for reverse-mode AD, via smoothing, which cannot be guaranteed to be unbiased in all cases. [Note: smoothing functionality for general programs not yet available.]\nWe do not yet support if statements with discrete random input. A workaround can be to use array indexing to express discrete random choices (see the random walk tutorial for an example).\nWe do not yet support non-real values as intermediate values (e.g. a function such as length(A[rand(Bernoulli(p))]) where A is an array of strings is in theory differentiable).\nWe do not support discrete random variables that are implicitly implemented using continuous random variables, e.g. rand() < p.\nWe support a limited assortment of discrete random variables: currently Bernoulli, Binomial, Geometric, Poisson, and Categorical from Distributions. We are working on increasing coverage across Distributions as well as other libraries providing discrete random samplers such as MeasureTheory.\nHigher-order differentiation is not supported.","category":"page"},{"location":"limitations.html","page":"Limitations","title":"Limitations","text":"StochasticAD is still in active development! PRs are welcome.","category":"page"},{"location":"index.html","page":"Overview","title":"Overview","text":"<img class=\"display-light-only\" src=\"images/path_skeleton.png\">\n<img class=\"display-dark-only\" src=\"images/path_skeleton_dark.png\">","category":"page"},{"location":"index.html#StochasticAD","page":"Overview","title":"StochasticAD","text":"","category":"section"},{"location":"index.html","page":"Overview","title":"Overview","text":"StochasticAD is an experimental, research package for automatic differentiation (AD) of stochastic programs. It implements AD algorithms for handling programs that can contain discrete randomness, based on the methodology developed in this NeurIPS 2022 paper.","category":"page"},{"location":"index.html#Introduction","page":"Overview","title":"Introduction","text":"","category":"section"},{"location":"index.html","page":"Overview","title":"Overview","text":"Derivatives are all about how functions are affected by a tiny change Œµ in their input. First, let's imagine perturbing the input of a deterministic, differentiable function such as f(p) = p^2 at p = 2.","category":"page"},{"location":"index.html","page":"Overview","title":"Overview","text":"using StochasticAD\nf(p) = p^2\nstochastic_triple(f, 2) # Feeds 2 + Œµ into f","category":"page"},{"location":"index.html","page":"Overview","title":"Overview","text":"The output tells us that if we change the input 2 by a tiny amount Œµ, the output of f will change by approximately 4Œµ. This is the case we're familiar with: we can get the value 4 by applying the chain rule, fracmathrmdmathrmd p p^2 = 2p = 4. Thinking in terms of tiny changes, the output above looks a lot like a dual number. But what happens with a discrete random function? Let's give it a try. ","category":"page"},{"location":"index.html","page":"Overview","title":"Overview","text":"import Random # hide\nRandom.seed!(4321) # hide\nusing StochasticAD, Distributions\nf(p) = rand(Bernoulli(p)) # 1 with probability p, 0 otherwise\nstochastic_triple(f, 0.5) # Feeds 0.5 + Œµ into f","category":"page"},{"location":"index.html","page":"Overview","title":"Overview","text":"The output of a Bernoulli variable cannot change by a tiny amount: it is either 0 or 1. But in the probabilistic world, there is another way to change by a tiny amount on average: jump by a large amount, with tiny probability. StochasticAD introduces a stochastic triple object, which generalizes dual numbers by including a third component to describe these perturbations. Here, the stochastic triple says that the original random output was 0, but given a small change Œµ in the input, the output will jump up to 1 with probability approximately 2Œµ.","category":"page"},{"location":"index.html","page":"Overview","title":"Overview","text":"Stochastic triples can be used to construct a new random program whose average is the derivative of the average of the original program. We simply propagate stochastic triples through the program via stochastic_triple, and then sum up the \"dual\" and \"triple\" components at the end via derivative_contribution. This process is packaged together in the function derivative_estimate. Let's try a crazier example, where we mix discrete and continuous randomness!","category":"page"},{"location":"index.html","page":"Overview","title":"Overview","text":"using StochasticAD, Distributions\nimport Random # hide\nRandom.seed!(1234) # hide\n\nfunction X(p)\n    a = p * (1 - p)\n    b = rand(Binomial(10, p))\n    c = 2 * b + 3 * rand(Bernoulli(p))\n    return a * c * rand(Normal(b, a))\nend\n\nst = @show stochastic_triple(X, 0.6) # sample a single stochastic triple at p = 0.6\n@show derivative_contribution(st) # which produces a single derivative estimate...\n\nsamples = [derivative_estimate(X, 0.6) for i in 1:1000] # many samples from derivative program\nderivative = mean(samples)\nuncertainty = std(samples) / sqrt(1000)\nprintln(\"derivative of ùîº[X(p)] = $derivative ¬± $uncertainty\")","category":"page"},{"location":"index.html#Index","page":"Overview","title":"Index","text":"","category":"section"},{"location":"index.html","page":"Overview","title":"Overview","text":"See the public API for a walkthrough of the API, and the tutorials on differentiating a random walk, a stochastic game of life, and a particle filter, and solving stochastic optimization and variational problems with discrete randomness. This is a prototype package with a number of limitations.","category":"page"},{"location":"devdocs.html#Developer-documentation","page":"Developer documentation","title":"Developer documentation","text":"","category":"section"},{"location":"devdocs.html","page":"Developer documentation","title":"Developer documentation","text":"Coming soon!","category":"page"}]
}
